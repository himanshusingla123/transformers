{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4372584c",
   "metadata": {},
   "source": [
    "### Time to slice and dice\n",
    "\n",
    "First we need to download and extract the data, which can be done with the wget and unzip commands:\n",
    "```bash\n",
    "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!unzip drugsCom_raw.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90306134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\":\"drugComTrain_raw.tsv\" , \"test\":\"drugComTest_raw.tsv\"}\n",
    "drug_dataset = load_dataset(\"csv\" , data_files=data_files , delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b715f2",
   "metadata": {},
   "source": [
    " In 🤗 Datasets, we can create a random sample by chaining the Dataset.shuffle() and Dataset.select() functions together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2cda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea06065",
   "metadata": {},
   "source": [
    "From this sample we can already see a few quirks in our dataset:\n",
    "\n",
    "1. The Unnamed: 0 column looks suspiciously like an anonymized ID for each patient.\n",
    "2. The condition column includes a mix of uppercase and lowercase labels.\n",
    "3. The reviews are of varying length and contain a mix of Python line separators (\\r\\n) as well as HTML character codes like &\\#039;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd679cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test the patient ID hypothesis for the Unnamed: 0 column, we can use the Dataset.unique() function to verify that the number of IDs matches the number of rows in each split:\n",
    "for split in drug_dataset.keys():\n",
    "    assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8289b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming of the column\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
    ")\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1180692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercasing of the condition column\n",
    "def lowercase_condition(example):\n",
    "    return {\"condition\": example[\"condition\"].lower()}\n",
    "\n",
    "def filter_nones(x):\n",
    "    return x[\"condition\"] is not None\n",
    "\n",
    "drug_dataset.filter(filter_nones)\n",
    "drug_dataset.map(lowercase_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.map(lowercase_condition)\n",
    "# Check that lowercasing worked\n",
    "drug_dataset[\"train\"][\"condition\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  creatng new columns\n",
    "def compute_review_length(example):\n",
    "    return {\"review_length\":len(example[\"review\"].split())}\n",
    "\n",
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "drug_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset[\"train\"].sort(\"review_length\")[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"]>30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91210c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "text = \"I&#039;m a transformer called BERT\"\n",
    "html.unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b7986d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'review': {1: 'one'}}, {'review': {2: 'two'}}, {'review': {3: 'three'}}, {'review': {4: 'four'}}, {'review': {5: 'five'}}, {'review': {6: 'six'}}]\n"
     ]
    }
   ],
   "source": [
    "a = [{1:\"one\"}, {2:\"two\"}, {3:\"three\"}, {4:\"four\"}, {5:\"five\"}, {6:\"six\"}]\n",
    "\n",
    "a = list(map(lambda x: {\"review\": x}, a))\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.map(lambda x: {\"review\":html.unescape(x[\"review\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11345bc",
   "metadata": {},
   "source": [
    "The Dataset.map() method takes a batched argument that, if set to True, causes it to send a batch of examples to the map function at once (the batch size is configurable but defaults to 1,000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388c752",
   "metadata": {},
   "source": [
    "list comprehensions are usually faster than executing the same code in a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b49d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_drug_dataset = drug_dataset.map(\n",
    "    lambda x: {\"review\" : [html.unescape(o) for o in x[\"review\"]]}, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea6bffe",
   "metadata": {},
   "source": [
    "<img src=\"image-3.png\" alt=\"Image 3\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d19612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc624a44",
   "metadata": {},
   "source": [
    "Using num_proc to speed up your processing is usually a great idea, as long as the function you are using is not already doing some kind of multiprocessing of its own.\n",
    "\n",
    "<img src=\"image-4.png\" alt=\"Image 4\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11777eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
    "\n",
    "\n",
    "def slow_tokenize_function(examples):\n",
    "    return slow_tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_split(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "result = tokenize_and_split(drug_dataset[\"train\"][0])\n",
    "print([len(inp) for inp in result[\"input_ids\"]])\n",
    "\n",
    "# tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True) # gives error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d3004",
   "metadata": {},
   "source": [
    "How **`Dataset.map()` interacts with tokenization and overflowing tokens**. Let’s go step by step on *why things broke* and *how both fixes solve it*:\n",
    "\n",
    "### 🔹 Problem Recap\n",
    "* You tokenize reviews with `return_overflowing_tokens=True`.\n",
    "* That means **one input row → possibly many tokenized chunks** (e.g., `128` tokens, then `49` tokens).\n",
    "* When you do this on the whole dataset with `batched=True`, the tokenizer may output **more rows than you input**.\n",
    "* Hugging Face datasets expect all columns (`condition`, `drugName`, `review`, etc.) to stay the **same length**.\n",
    "* But after tokenization, your new `input_ids` column has more rows than the old `condition` column → 💥 ArrowInvalid error.\n",
    "\n",
    "### ✅ Solution 1: Drop old columns\n",
    "* Here you **throw away the original dataset columns** (`condition`, `drugName`, `review`, etc.).\n",
    "* You keep only the tokenizer output (`input_ids`, `attention_mask`, etc.).\n",
    "* This is fine if you **don’t need the original text/metadata anymore**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution 1\n",
    "tokenized_dataset = drug_dataset.map(\n",
    "    tokenize_and_split,\n",
    "    batched=True,\n",
    "    remove_columns=drug_dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcc49f4",
   "metadata": {},
   "source": [
    "### ✅ Solution 2: Keep old columns with `overflow_to_sample_mapping`\n",
    "\n",
    "* The tokenizer returns an extra field `overflow_to_sample_mapping`.\n",
    "  Example: `[0, 0, 1, 2, 2, 2, 3]` → means the 2nd and 3rd tokenized chunks came from sample `0`, and three chunks came from sample `2`.\n",
    "* You **use that mapping to duplicate the metadata** (`condition`, `drugName`, etc.) so they match the new tokenized chunks.\n",
    "* Now all columns have equal length → ✅ no Arrow error.\n",
    "* Best if you want to **retain original fields** for analysis, grouping, or error-checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution 2\n",
    "def tokenize_and_split(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    # Mapping: new_index -> old_index\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    # Repeat old data to align with new chunks\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3902e",
   "metadata": {},
   "source": [
    "To enable the conversion between various third-party libraries, 🤗 Datasets provides a *Dataset.set_format()* function. This function only changes the output format of the dataset, so you can easily switch to another format without affecting the underlying data format, which is Apache Arrow. The formatting is done in place. To demonstrate, let’s convert our dataset to Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = drug_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c94c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = (\n",
    "    train_df[\"condition\"]\n",
    "    .value_counts()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"condition\", \"count\": \"frequency\"})\n",
    ")\n",
    "frequencies.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e2c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "freq_dataset = Dataset.from_pandas(frequencies)\n",
    "freq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e36171",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3da21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "# Rename the default \"test\" split to \"validation\"\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "# Add the \"test\" set to our `DatasetDict`\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b5888f",
   "metadata": {},
   "source": [
    "Datasets provides three main functions to save your dataset in different formats:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e102ac1",
   "metadata": {},
   "source": [
    "<img src=\"image-5.png\" alt=\"image 5\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean.save_to_disk(\"drug-reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"drug-reviews\")\n",
    "drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"drug-reviews-{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e8dfd",
   "metadata": {},
   "source": [
    "## Big data? 🤗 Datasets to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92068ecb",
   "metadata": {},
   "source": [
    "Exactly 👍 you’ve landed on one of the **killer features** of 🤗 Datasets:\n",
    "\n",
    "👉 **Memory mapping (zero-copy datasets) + streaming**\n",
    "\n",
    "This is why you can load something as huge as **The Pile (825 GB!)** on a normal laptop without instantly running out of RAM.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 How it works\n",
    "\n",
    "### 1. **Memory mapping (Arrow format)**\n",
    "\n",
    "* When you `load_dataset()`, 🤗 Datasets stores the data in **Apache Arrow** format on disk.\n",
    "* Instead of reading the whole dataset into RAM, it creates a **memory-mapped file**.\n",
    "* This means:\n",
    "\n",
    "  * Data stays on disk.\n",
    "  * Only the portion you actually access (`dataset[0]`, a column, or a batch) gets read into RAM.\n",
    "  * Huge datasets behave like small ones.\n",
    "\n",
    "✅ That’s why your `pubmed_dataset` (\\~20 GB) only used \\~5.6 GB of RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ac6d3",
   "metadata": {},
   "source": [
    "### 2. **Streaming**\n",
    "\n",
    "For datasets that are **larger than your disk space** (like 825 GB Pile), you can also use `streaming=True`:\n",
    "\n",
    "* The data is **never fully downloaded** or **fully loaded in RAM**.\n",
    "* Perfect for web-scale corpora (like The Pile, Common Crawl, LAION, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"https://the-eye.eu/public/AI/pile_preliminary_components/github.jsonl.zst\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# This returns an **iterable dataset**.\n",
    "# You can loop over it like a generator:\n",
    "\n",
    "for sample in dataset.take(5):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd9f79",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔹 Comparing the three modes\n",
    "\n",
    "| Mode                        | Storage                         | RAM usage                    | Access pattern           | Example use case              |\n",
    "| --------------------------- | ------------------------------- | ---------------------------- | ------------------------ | ----------------------------- |\n",
    "| **In-memory**               | Entire dataset in RAM           | High (equals dataset size)   | Random access, very fast | Small datasets (<1–2 GB)      |\n",
    "| **Memory-mapped (default)** | Stored in Arrow cache on disk   | Small (only accessed chunks) | Random access (fast)     | Medium datasets (10 GB–1 TB)  |\n",
    "| **Streaming**               | Remote files (HTTP, S3, HF Hub) | Tiny (just current batch)    | Sequential only          | Very large datasets (100 GB+) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6435cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "print(f\"Dataset size in bytes: {pubmed_dataset.dataset_size}\")\n",
    "size_gb = pubmed_dataset.dataset_size / (1024**3)\n",
    "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2becb32",
   "metadata": {},
   "source": [
    "## 🔹 Example: Measuring RAM vs disk size\n",
    "\n",
    "\n",
    "✅ Result: dataset is \\~20 GB, but RAM usage \\~5 GB.\n",
    "If you use `streaming=True`, RAM usage will drop even further (basically near-constant).\n",
    "\n",
    "---\n",
    "\n",
    "⚡ **Takeaway**: Hugging Face Datasets gives you Big Data handling out of the box:\n",
    "\n",
    "* You can work with corpora much larger than your RAM (memory mapping).\n",
    "* You can even work with datasets larger than your disk (streaming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s run a little speed test by iterating over all the elements in the PubMed Abstracts dataset:\n",
    "\n",
    "import timeit\n",
    "\n",
    "code_snippet = \"\"\"batch_size = 1000\n",
    "\n",
    "for idx in range(0, len(pubmed_dataset), batch_size):\n",
    "    _ = pubmed_dataset[idx:idx + batch_size]\n",
    "\"\"\"\n",
    "\n",
    "time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())\n",
    "print(\n",
    "    f\"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in \"\n",
    "    f\"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef4977",
   "metadata": {},
   "source": [
    "1. Here we’ve used Python’s timeit module to measure the execution time taken by code_snippet. \n",
    "2. You’ll typically be able to iterate over a dataset at speed of a few tenths of a GB/s to several GB/s. \n",
    "3. This works great for the vast majority of applications, but sometimes you’ll have to work with a dataset that is too large to even store on your laptop’s hard drive. \n",
    "4. For example, if we tried to download the Pile in its entirety, we’d need 825 GB of free disk space! To handle these cases, 🤗 Datasets provides a streaming feature that allows us to download and access elements on the fly, without needing to download the whole dataset. Let’s take a look at how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e80341",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_dataset_streamed = load_dataset(\n",
    "    \"json\", data_files=data_files, split=\"train\", streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287d3d1",
   "metadata": {},
   "source": [
    "Instead of the familiar Dataset that we’ve encountered elsewhere in this chapter, the object returned with streaming=True is an IterableDataset. As the name suggests, to access the elements of an IterableDataset we need to iterate over it. We can access the first element of our streamed dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(pubmed_dataset_streamed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7b158",
   "metadata": {},
   "source": [
    "The elements from a streamed dataset can be processed on the fly using IterableDataset.map(), which is useful during training if you need to tokenize the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x[\"text\"]))\n",
    "next(iter(tokenized_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e3469",
   "metadata": {},
   "source": [
    "You can also shuffle a streamed dataset using IterableDataset.shuffle(), but unlike Dataset.shuffle() this only shuffles the elements in a predefined buffer_size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b903e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)\n",
    "next(iter(shuffled_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e09ed",
   "metadata": {},
   "source": [
    "1. In this example, we selected a random example from the first 10,000 examples in the buffer. \n",
    "2. Once an example is accessed, its spot in the buffer is filled with the next example in the corpus (i.e., the 10,001st example in the case above). You can also select elements from a streamed dataset using the IterableDataset.take() and IterableDataset.skip() functions, which act in a similar way to Dataset.select(). \n",
    "3. For example, to select the first 5 examples in the PubMed Abstracts dataset we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31408be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_head = pubmed_dataset_streamed.take(5)\n",
    "list(dataset_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35e717",
   "metadata": {},
   "source": [
    "Similarly, you can use the IterableDataset.skip() function to create training and validation splits from a shuffled dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4915ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the first 1,000 examples and include the rest in the training set\n",
    "train_dataset = shuffled_dataset.skip(1000)\n",
    "# Take the first 1,000 examples for the validation set\n",
    "validation_dataset = shuffled_dataset.take(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7472aa2d",
   "metadata": {},
   "source": [
    "1. Let’s round out our exploration of dataset streaming with a common application: combining multiple datasets together to create a single corpus.\n",
    "2. 🤗 Datasets provides an interleave_datasets() function that converts a list of IterableDataset objects into a single IterableDataset, where the elements of the new dataset are obtained by alternating among the source examples.  \n",
    "3. This function is especially useful when you’re trying to combine large datasets, so as an example let’s stream the FreeLaw subset of the Pile, which is a 51 GB dataset of legal opinions from US courts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "law_dataset_streamed = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "next(iter(law_dataset_streamed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c20f8f8",
   "metadata": {},
   "source": [
    "1. This dataset is large enough to stress the RAM of most laptops, yet we’ve been able to load and access it without breaking a sweat! \n",
    "2. Let’s now combine the examples from the FreeLaw and PubMed Abstracts datasets with the interleave_datasets() function.\n",
    "3. Here we’ve used the islice() function from Python’s itertools module to select the first two examples from the combined dataset, and we can see that they match the first examples from each of the two source datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c962451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from datasets import interleave_datasets\n",
    "\n",
    "combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])\n",
    "list(islice(combined_dataset, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58817767",
   "metadata": {},
   "source": [
    "Finally, if you want to stream the Pile in its 825 GB entirety, you can grab all the prepared files as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e46dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://the-eye.eu/public/AI/pile/\"\n",
    "data_files = {\n",
    "    \"train\": [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(30)],\n",
    "    \"validation\": base_url + \"val.jsonl.zst\",\n",
    "    \"test\": base_url + \"test.jsonl.zst\",\n",
    "}\n",
    "pile_dataset = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
    "next(iter(pile_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a606d",
   "metadata": {},
   "source": [
    "## Crate your own dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2102f4",
   "metadata": {},
   "source": [
    "Now that we have our access token, let’s create a function that can download all the issues from a GitHub repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb479905",
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = xxx  # Copy your GitHub token here\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8fd11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "\n",
    "def fetch_issues(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"datasets\",\n",
    "    num_issues=10_000,\n",
    "    rate_limit=5_000,\n",
    "    issues_path=Path(\".\"),\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100  # Number of issues to return per page\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        # Query with state=all to get both open and closed issues\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []  # Flush batch for next time period\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.DataFrame.from_records(all_issues)\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
    "    print(\n",
    "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e3f4f",
   "metadata": {},
   "source": [
    "Now when we call fetch_issues() it will download all the issues in batches to avoid exceeding GitHub’s limit on the number of requests per hour; the result will be stored in a repository_name-issues.jsonl file, where each line is a JSON object the represents an issue. Let’s use this function to grab all the issues from 🤗 Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01c755",
   "metadata": {},
   "source": [
    "Once the issues are downloaded we can load them locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19201d",
   "metadata": {},
   "source": [
    "But why are there several thousand issues when the Issues tab of the 🤗 Datasets repository only shows around 1,000 issues in total "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47027162",
   "metadata": {},
   "source": [
    "GitHub’s REST API v3 considers every pull request an issue, but not every issue is a pull request. For this reason, “Issues” endpoints may return both issues and pull requests in the response. You can identify pull requests by the pull_request key. Be aware that the id of a pull request returned from “Issues” endpoints will be an issue id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = issues_dataset.shuffle(seed=666).select(range(3))\n",
    "\n",
    "# Print out the URL and pull request entries\n",
    "for url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n",
    "    print(f\">> URL: {url}\")\n",
    "    print(f\">> Pull request: {pr}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cc28b",
   "metadata": {},
   "source": [
    "Here we can see that each pull request is associated with various URLs, while ordinary issues have a None entry. We can use this distinction to create a new is_pull_request column that checks whether the pull_request field is None or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset = issues_dataset.map(\n",
    "    lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f72f9",
   "metadata": {},
   "source": [
    "The GitHub REST API provides a Comments endpoint that returns all the comments associated with an issue number. Let’s test the endpoint to see what it returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(issue_number):\n",
    "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return [r[\"body\"] for r in response.json()]\n",
    "\n",
    "\n",
    "# Test our function works as expected\n",
    "get_comments(2792)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78469afa",
   "metadata": {},
   "source": [
    "Let’s use Dataset.map() to add a new comments column to each issue in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3feebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on your internet connection, this can take a few minutes...\n",
    "issues_with_comments_dataset = issues_dataset.map(\n",
    "    lambda x: {\"comments\": get_comments(x[\"number\"])}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac880ea4",
   "metadata": {},
   "source": [
    "#### Upload a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b33a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_with_comments_dataset.push_to_hub(\"github-issues\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
