**Reformer** uses to handle **very long sequences** efficiently. Letâ€™s break down **axial positional encodings** in simple terms:

---

### ğŸ”¹ Background: Why Positional Encodings?

Transformers donâ€™t have a sense of order by default (they just see tokens as a set).
So we add **positional encodings (PEs)** â†’ vectors that tell the model **â€œthis token is at position 1, this token at position 2, etc.â€**

Normally:

* If sequence length = `l`
* Hidden dimension = `d`
* PE matrix size = `l Ã— d`

ğŸ‘‰ For long sequences (say `l = 1,000,000`, `d = 1024`), thatâ€™s **1 billion values** â†’ too big for GPU.

---

### ğŸ”¹ Reformerâ€™s Solution: Axial Positional Encodings

Instead of storing a **huge matrix**, Reformer **factorizes it into two smaller ones**.

We split:

* Sequence length â†’ `l = lâ‚ Ã— lâ‚‚`
* Hidden dimension â†’ `d = dâ‚ + dâ‚‚`

Now we build:

* `Eâ‚` â†’ size `(lâ‚ Ã— dâ‚)`
* `Eâ‚‚` â†’ size `(lâ‚‚ Ã— dâ‚‚)`

The position `j` in the sequence is encoded as:

```
PE[j] = concat( Eâ‚[j % lâ‚], Eâ‚‚[j // lâ‚] )
```

---

### ğŸ”¹ Example

Suppose:

* Sequence length `l = 12`
* Hidden dim `d = 6`

We choose:

* `lâ‚ = 4`, `lâ‚‚ = 3`  (since `4 Ã— 3 = 12`)
* `dâ‚ = 3`, `dâ‚‚ = 3`

So:

* `Eâ‚` is a `4 Ã— 3` matrix
* `Eâ‚‚` is a `3 Ã— 3` matrix

Now:

* Position `j = 7` â†’

  * `j % lâ‚ = 7 % 4 = 3` â†’ take row 3 of `Eâ‚`
  * `j // lâ‚ = 7 // 4 = 1` â†’ take row 1 of `Eâ‚‚`
  * Final encoding = **concat(row3(Eâ‚), row1(Eâ‚‚))**

---

### ğŸ”¹ Why is this useful?

* Memory is much smaller (`lâ‚Â·dâ‚ + lâ‚‚Â·dâ‚‚` instead of `lÂ·d`).
* Still uniquely encodes positions across very long sequences.
* Works especially well with **LSH attention** (Reformerâ€™s other trick).

---

âœ… In short: **Axial PEs = breaking a huge position matrix into 2D factors**, like reshaping a 1D sequence into a grid, and then encoding row + column instead of each position separately.
