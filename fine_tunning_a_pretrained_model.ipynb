{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efc4b8d",
   "metadata": {},
   "source": [
    "Each of the main sections in this chapter will teach you something different:\n",
    "* Section 1: Learn modern data preprocessing techniques and efficient dataset handling\n",
    "* Section 2: Master the powerful Trainer API with all its latest features\n",
    "* Section 3: Implement training loops from scratch and understand distributed training with Accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5838c0d",
   "metadata": {},
   "source": [
    "### Section 1: Learn modern data preprocessing techniques and efficient dataset handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54482e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer , AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "\n",
    "batch = tokenizer(sequences , padding=True , truncation= True , return_tensors=\"pt\")\n",
    "\n",
    "batch[\"labels\"] = torch.tensor([1,1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89095820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\" , \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc22592",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d57638",
   "metadata": {},
   "source": [
    "#### Preprocessing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e8ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentence_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentence_2 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2cf5e7",
   "metadata": {},
   "source": [
    "**One way to preprocess the training dataset**\n",
    "This works well, but it has the disadvantage of returning a dictionary (with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists). It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the ðŸ¤— Datasets library are Apache Arrow files stored on the disk, so you only keep the samples you ask for loaded in memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae6db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    paddinng=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec26c96",
   "metadata": {},
   "source": [
    "**Second way to preprocess the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ea565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = raw_datasets.map(tokenize_function , batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e966e69d",
   "metadata": {},
   "source": [
    "## **Dynamic Padding**\n",
    "The function that is responsible for dynamic padding is the `DataCollatorWithPadding` from the ðŸ¤— Transformers library. It automatically pads your input sequences to the maximum length of the batch, ensuring that all sequences in the batch have the same length. This is particularly useful when working with variable-length sequences, as it allows you to efficiently process batches of data without wasting memory on unnecessary padding.\n",
    "\n",
    "To use dynamic padding, simply pass an instance of `DataCollatorWithPadding` to your Trainer or DataLoader. Here's an example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc10e7",
   "metadata": {},
   "source": [
    "The function that is responsible for putting together samples inside a batch is called a collate function. Itâ€™s an argument you can pass when you build a DataLoader, the default being a function that will just convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This wonâ€™t be possible in our case since the inputs we have wonâ€™t all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if youâ€™re training on a TPU it can cause problems â€” TPUs prefer fixed shapes, even when that requires extra padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10dfa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ea8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tokenized_dataset[\"train\"][:8]\n",
    "\n",
    "samples = {k:v for k , v in samples.items() if k not in [\"idx\" , \"sentence1\" , \"sentence2\"]}\n",
    "\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator(samples)\n",
    "{k:v.shape for k ,v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c420d7",
   "metadata": {},
   "source": [
    "## **Fine-tuning a model with the Trainer API**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34090799",
   "metadata": {},
   "source": [
    " Transformers provides a Trainer class to help you fine-tune any of the pretrained models it provides on your dataset with modern best practices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"] , example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = raw_datasets.map(tokenize_function , batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670aa1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments , AutoModelForSequenceClassification\n",
    "training_args =  TrainingArguments(\"test-trainer\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint , num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59af4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset= tokenized_dataset[\"train\"],\n",
    "    eval_dataset= tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ced58",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_dataset[\"validation\"])\n",
    "print(predictions.predictions.shape , predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55186dcf",
   "metadata": {},
   "source": [
    "To transform logits i.e (predictions.predictions) them into predictions that we can compare to our labels, we need to take the index with the maximum value on the second axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2665fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "preds = np.argmax(predictions.predictions , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"glue\" , \"mrpc\")\n",
    "metric.compute(predictions = preds , references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68e321",
   "metadata": {},
   "source": [
    "**Wrapping everything we get**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdaeca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\",\"mrpc\")\n",
    "    logits , labels = eval_preds\n",
    "    predictions = np.argmax(logits , axis = -1)\n",
    "    return metric.compute(predictions=predictions , references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", eval_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f059eda",
   "metadata": {},
   "source": [
    "### A full training loop\n",
    "1. Prepare for training\n",
    "2. The trainig loop\n",
    "3. The evaluation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eedd1bc",
   "metadata": {},
   "source": [
    "Before actually writing our training loop, we will need to define a few objects. The first ones are the dataloaders we will use to iterate over batches. But before we can define those dataloaders, we need to apply a bit of postprocessing to our tokenized_datasets, to take care of some things that the Trainer did for us automatically. Specifically, we need to:\n",
    "\n",
    "1. Remove the columns corresponding to values the model does not expect (like the sentence1 and sentence2 columns).\n",
    "2. Rename the column label to labels (because the model expects the argument to be named labels).\n",
    "3. Set the format of the datasets so they return PyTorch tensors instead of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be80f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"sentence1\" , \"sentence2\",\"idx\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_columns([\"label\" , \"labels\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f1df3c",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77826d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"] , shuffle=True , batch_size=8 , collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_dataset[\"validation\"] , batch_size=8 , collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k:v.shape for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint , num_labels= 2)\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e7aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters() , lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c65deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer = optimizer,\n",
    "    num_training_steps = num_training_steps,\n",
    "    num_warmup_steps=0\n",
    ")\n",
    "print(num_training_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172e035",
   "metadata": {},
   "source": [
    "check if GPU is accessible or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f35d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(\"device\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff4b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device(type=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf956380",
   "metadata": {},
   "source": [
    "To get some sense of when training will be finished, we add a progress bar over our number of training steps, using the tqdm library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807529f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417bd1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f9c416",
   "metadata": {},
   "source": [
    "### **Supercharge your training loop with ðŸ¤— Accelerate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b502bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "def training_function():\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "    train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
    "        train_dataloader, eval_dataloader, model, optimizer\n",
    "    )\n",
    "\n",
    "    num_epochs = 3\n",
    "    num_training_steps = num_epochs * len(train_dl)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dl:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "notebook_launcher(training_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03282891",
   "metadata": {},
   "source": [
    "### **Understanding Learning Curves**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9b21e7",
   "metadata": {},
   "source": [
    "Learning curves are visual representations of your modelâ€™s performance metrics over time during training. The two most important curves to monitor are:\n",
    "\n",
    "1. Loss curves: Show how the modelâ€™s error (loss) changes over training steps or epochs\n",
    "2. Accuracy curves: Show the percentage of correct predictions over training steps or epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4236bf6",
   "metadata": {},
   "source": [
    "Loss Curves\n",
    "The loss curve shows how the modelâ€™s error decreases over time. In a typical successful training run, youâ€™ll see a curve similar to the one below:\n",
    "\n",
    "<img src=\"image-1.png\" alt=\"alt text\" width=\"300\" height=\"200\">\n",
    "\n",
    "1. High initial loss: The model starts without optimization, so predictions are initially poor\n",
    "2. Decreasing loss: As training progresses, the loss should generally decrease\n",
    "3. Convergence: Eventually, the loss stabilizes at a low value, indicating that the model has learned the patterns in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b83644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"transformer-fine-tuning\", name=\"bert-mrpc-analysis\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,  # Log metrics every 10 steps\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    report_to=\"wandb\",  # Send logs to Weights & Biases\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train and automatically log metrics\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107f520f",
   "metadata": {},
   "source": [
    "Accuracy Curves\n",
    "The accuracy curve shows the percentage of correct predictions over time. Unlike loss curves, accuracy curves should generally increase as the model learns and can typically include more steps than the loss curve.\n",
    "\n",
    "<img src=\"image-2.png\" alt=\"alt text\" width=\"300\" height=\"200\">\n",
    "\n",
    "1. Start low: Initial accuracy should be low, as the model has not yet learned the patterns in the data\n",
    "2. Increase with training: Accuracy should generally improve as the model learns if it is able to learn the patterns in the data\n",
    "3. May show plateaus: Accuracy often increases in discrete jumps rather than smoothly, as the model makes predictions that are close to the true labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b774f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daebc80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
